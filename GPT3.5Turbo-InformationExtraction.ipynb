{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d11822a",
   "metadata": {},
   "source": [
    "# GPT 3.5 Turbo - Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2473bc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucid\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\lucid\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "C:\\Users\\lucid\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import time \n",
    "import pdfkit\n",
    "import re\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "openai.organization = os.environ.get(\"OPENAI_ORGANIZATION\")\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d70b5568",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eda83184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Model model id=gpt-3.5-turbo at 0x1d76cca5220> JSON: {\n",
       "  \"created\": 1677610602,\n",
       "  \"id\": \"gpt-3.5-turbo\",\n",
       "  \"object\": \"model\",\n",
       "  \"owned_by\": \"openai\",\n",
       "  \"parent\": null,\n",
       "  \"permission\": [\n",
       "    {\n",
       "      \"allow_create_engine\": false,\n",
       "      \"allow_fine_tuning\": false,\n",
       "      \"allow_logprobs\": true,\n",
       "      \"allow_sampling\": true,\n",
       "      \"allow_search_indices\": false,\n",
       "      \"allow_view\": true,\n",
       "      \"created\": 1683753011,\n",
       "      \"group\": null,\n",
       "      \"id\": \"modelperm-YO9wdQnaovI4GD1HLV59M0AV\",\n",
       "      \"is_blocking\": false,\n",
       "      \"object\": \"model_permission\",\n",
       "      \"organization\": \"*\"\n",
       "    }\n",
       "  ],\n",
       "  \"root\": \"gpt-3.5-turbo\"\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.Model.retrieve('gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "979478aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "edmundsreviews = pd.read_csv('Edmunds_CustomerReviews.csv', encoding = \"utf-16\")\n",
    "edmundsreviews.drop('Unnamed: 0', inplace = True, axis = 1)\n",
    "edmundsreviews.head()\n",
    "\n",
    "youtubereviews = pd.read_csv('YoutubeComments_40_InfoRich.csv')\n",
    "youtubereviews.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "\n",
    "carsreviews = pd.read_csv('CarsCustomerReviews.csv')\n",
    "carsreviews.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "\n",
    "kbbreviews = pd.read_csv('KBBCustomerReviews.csv')\n",
    "kbbreviews.drop('Unnamed: 0', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a230d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ChatGPT API\n",
    "def trash_to_treasure(df):\n",
    "    dflist = []\n",
    "    #For debug purposes - API issues, rate limits etc. \n",
    "    error = []\n",
    "    error_index = []\n",
    "    counter = 0\n",
    "    comments = df[\"Review\"].tolist()\n",
    "    for comment in tqdm(comments):\n",
    "        segment = df.iloc[counter]['Segment']\n",
    "        model = df.iloc[counter]['Model']\n",
    "        make = df.iloc[counter]['Make']\n",
    "        year = df.iloc[counter]['Year']\n",
    "        try:\n",
    "            content = openai.ChatCompletion.create(\n",
    "            model = 'gpt-3.5-turbo',\n",
    "            messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that helps answer questions and perform the specific task \"\n",
    "         + \"being asked.\"},\n",
    "        {\"role\": \"user\", \"content\":\" We are attempting to understand what vehicle features customers truly care about and what \"\\\n",
    "         + \"the pain points are based on customer comments. Note that the comments are related to \"\\\n",
    "         + \"automobile reviews hence pay more attention to certain words like Blind Spot Detection, Braking, Steering, \"\\\n",
    "         + \"Acceleration and other Car related features if they exist in the comments. Also make sure to focus only on the \"\\\n",
    "         + \"present vehicle being talked about and not on thoughts on similar vehicles owned at a prior time. With that in mind, \"\\\n",
    "         + \"what are the positives and the negative takeaways the customers had based on the following comment? Did the customer \"\\\n",
    "         + \"have any specific wishes? If so, list those out as part of a separate wishlist. Note that if the customer had a wish \"\\\n",
    "         + \"or a requested feature or doesn't list any relevant vehicle features, do not consider that as a positive or a negative. \"\\\n",
    "         + \"For each comment, formulate the answer with Positives, Negatives and Wishlist categories with short keywords per bullet. \"\\\n",
    "         + \"Do not number the bullet points. Summarize each bullet into specific keywords relating to vehicle \"\\\n",
    "         + \"features and make it a priority keep it as short as possible, around 3 - 4 words per bullet. Prefer concise words representing vehicle features over longer phrases. Do not add anything\"\\\n",
    "         + \" else to the response. Do not add an overall or summary section Wishlist. The following comment is in regards to the vehicle \"+ df.iloc[counter]['Make'] + ' ' + df.iloc[counter]['Model'] +\".  Comment : \"+comment}\n",
    "         ],\n",
    "        temperature = 1,\n",
    "        max_tokens= 300,\n",
    "        frequency_penalty = 0,\n",
    "        presence_penalty = 0,\n",
    "        #n=3\n",
    "            ).get(\"choices\")[0]['message']['content'].split('\\n')\n",
    "    \n",
    "    #Data Cleaning, Data Formatting into a consistent response programatically - Can only control output to an extent with \n",
    "    #prompt engineering. \n",
    "    #Blank Space Removal, Replacing Bullets with Blanks, Capturing different occurences of Categories to slice them into \n",
    "    #appropriate data formats such as list of tuples to enable creation of the final dataframe. \n",
    "            if '' in content:\n",
    "                content.remove('')\n",
    "            ret = [x.replace('- ', '') for x in content]\n",
    "            posstring = ''\n",
    "            for x in ret:\n",
    "                find = re.findall(r'^Positives:.*', x)\n",
    "                if find != []:\n",
    "                    posstring = posstring + find[0]\n",
    "                    break\n",
    "            \n",
    "            negstring = ''\n",
    "            for x in ret:\n",
    "                find = re.findall(r'^Negatives:.*', x)\n",
    "                if find != []:\n",
    "                    negstring = negstring + find[0]\n",
    "                    break\n",
    "            \n",
    "            wishstring = ''\n",
    "            for x in ret:\n",
    "                find = re.findall(r'^Wishlist:.*', x)\n",
    "                if find != []:\n",
    "                    wishstring = wishstring + find[0]\n",
    "                    break\n",
    "        \n",
    "            #segment = df.iloc[counter]['Segment']\n",
    "            #model = df.iloc[counter]['Model']\n",
    "            #make = df.iloc[counter]['Make']\n",
    "            #year = df.iloc[counter]['Year']\n",
    "            try:\n",
    "                p = ret.index(posstring)\n",
    "                n = ret.index(negstring)\n",
    "                w = ret.index(wishstring)\n",
    "                positives = ', '.join(ret[p+1:n])\n",
    "                negatives = ', '.join(ret[n+1:w])\n",
    "                wishlist = ', '.join(ret[w+1:])\n",
    "            except:\n",
    "            #This exception comes into play in case there's no relevant information in any of the categories or in context to \n",
    "            #Nissan vehicles where ChatGPT outputs a warning. This is done to maintain indexing and subsequent metadata rather\n",
    "            #than skipping the iteration entirely. \n",
    "                positives = ''\n",
    "                negatives = ''\n",
    "                wishlist = ''\n",
    "                print('Trigger')\n",
    "                #dflist.append((counter, segment, model, make, year, positives, negatives, wishlist))\n",
    "                #continue\n",
    "    \n",
    "            dflist.append((counter, segment, model, make, year, positives, negatives, wishlist))\n",
    "            counter = counter + 1\n",
    "            time.sleep(0.5)\n",
    "    \n",
    "    #This exception is used in case of any rate errors or API connectivity issues so that continuation is possible from the \n",
    "    #remaining comments. \n",
    "        except Exception as e:\n",
    "            error.append(e)\n",
    "            error_index.append(counter)\n",
    "            positives = ''\n",
    "            negatives = ''\n",
    "            wishlist = ''\n",
    "            print('Trigger_2')\n",
    "            dflist.append((counter, segment, model, make, year, positives, negatives, wishlist))\n",
    "            counter = counter + 1\n",
    "            continue\n",
    "            #return (dflist, error, error_index) \n",
    "    return (dflist, error, error_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376f5034",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Depending on the run, you would get problematic indexes. These can be found at t1[2]. Rerun the function with trash_to_treasure(t1[2]) and create dataframe, export it and then concatenate it. \n",
    "#This process has been repeated multiple times to get the final output for the first pass. The Original comments aren't needed \n",
    "#after this point. This method was used to generate multiple files which have then been concatenated and processed. \n",
    "#t1 = trash_to_treasure(youtubereviews)\n",
    "#t1 = trash_to_treasure(edmundsreviews)\n",
    "#t1 = trash_to_treasure(kbbreviews)\n",
    "t1 = trash_to_treasure(carsreviews.iloc[t1[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a374d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Due to long processing times, the process was done in batches.\n",
    "#It can be processed as follows.\n",
    "#if t1[2] is not empty, re-run the above function with just those indexes \n",
    "#Eg : trash_to_treasure(kbbreviews.iloc[t1[2]])\n",
    "df = pd.DataFrame(t1[0], columns = [\"Counter\", \"Segment\", \"Model\", \"Make\", \"Year\", \"Positives\", \"Negatives\", \"Wishlist\"])\n",
    "df.drop('Counter', axis = 1, inplace = True)\n",
    "df = df.drop_duplicates()\n",
    "df.to_csv('CarsExtractPFinal.csv')\n",
    "#df.to_csv('filename.csv') or df.to_excel('filename.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8819ae71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Due to long processing times, the process was done in batches. \n",
    "df = pd.read_excel('EdmundsextractP1.xlsx')\n",
    "df2 = pd.read_excel('EdmundsextractP2.xlsx')\n",
    "df3 = pd.read_excel('EdmundsextractP3.xlsx')\n",
    "df4 = pd.read_excel('EdmundsextractP4.xlsx')\n",
    "df5 = pd.read_excel('EdmundsextractP5.xlsx')\n",
    "df6 = pd.read_excel('EdmundsextractP6.xlsx')\n",
    "df7 = pd.read_excel('EdmundsextractP2_5.xlsx')\n",
    "df8 = pd.read_excel('EdmundsextractPLast.xlsx')\n",
    "df9 = pd.read_excel('EdmundsextractPErrorIndexes.xlsx')\n",
    "df10 = pd.read_excel('EdmundsextractErrorLast.xlsx')\n",
    "df_final = pd.concat([df, df2, df7, df3, df4, df5, df6, df8, df9, df10])\n",
    "df_final = df_final.reset_index()\n",
    "df_final.drop(['index', 'Unnamed: 0', 'Counter'], axis = 1, inplace = True)\n",
    "#Don't drop duplicates\n",
    "#df_final = df_final.drop_duplicates()\n",
    "df_final['Positives'] = df_final['Positives'].fillna('None mentioned')\n",
    "df_final['Negatives'] = df_final['Negatives'].fillna('None mentioned')\n",
    "df_final['Wishlist'] = df_final['Wishlist'].fillna('None mentioned')\n",
    "df_final.to_csv('EdmundsPass1.csv')\n",
    "\n",
    "df = pd.read_csv('YoutubeExtractP1.csv')\n",
    "df2 = pd.read_csv('YoutubeExtractP2.csv')\n",
    "df3 = pd.read_csv('YoutubeExtractP3.csv')\n",
    "df4 = pd.read_csv('YoutubeExtractP4.csv')\n",
    "df5 = pd.read_csv('YoutubeExtractP5.csv')\n",
    "df7 = pd.read_csv('YoutubeExtractP6.csv')\n",
    "df6 = pd.read_csv('YoutubeExtractPLast.csv')\n",
    "df_final = pd.concat([df, df2, df3, df4, df5, df7, df6])\n",
    "df_final = df_final.reset_index()\n",
    "df_final.drop(['index', 'Unnamed: 0', 'Counter'], axis = 1, inplace = True)\n",
    "#Don't drop duplicates\n",
    "#df_final = df_final.drop_duplicates()\n",
    "df_final['Positives'] = df_final['Positives'].fillna('None mentioned')\n",
    "df_final['Negatives'] = df_final['Negatives'].fillna('None mentioned')\n",
    "df_final['Wishlist'] = df_final['Wishlist'].fillna('None mentioned')\n",
    "df_final.to_csv('YoutubePass1.csv')\n",
    "\n",
    "df = pd.read_csv('KBBExtractP1.csv')\n",
    "df2 = pd.read_csv('KBBExtractP1_5.csv')\n",
    "df3 = pd.read_csv('KBBExtractP2.csv')\n",
    "df4 = pd.read_csv('KBBExtractP2_5.csv')\n",
    "df_final = pd.concat([df, df2, df3, df4])\n",
    "df_final = df_final.reset_index()\n",
    "df_final.drop(['index', 'Unnamed: 0'], axis = 1, inplace = True)\n",
    "#Don't drop duplicates\n",
    "#df_final = df_final.drop_duplicates()\n",
    "df_final['Positives'] = df_final['Positives'].fillna('None mentioned')\n",
    "df_final['Negatives'] = df_final['Negatives'].fillna('None mentioned')\n",
    "df_final['Wishlist'] = df_final['Wishlist'].fillna('None mentioned')\n",
    "df_final.to_csv('KBBPass1.csv')\n",
    "\n",
    "df = pd.read_csv('CarsExtractP1.csv')\n",
    "df2 = pd.read_csv('CarsExtractP1_5.csv')\n",
    "df3 = pd.read_csv('CarsExtractP1_75.csv')\n",
    "df4 = pd.read_csv('CarsExtractP2.csv')\n",
    "df5 = pd.read_csv('CarsExtractP2_25.csv')\n",
    "df6 = pd.read_csv('CarsExtractP2_5.csv')\n",
    "df7 = pd.read_csv('CarsExtractP2_75.csv')\n",
    "df8 = pd.read_csv('CarsExtractP3.csv')\n",
    "df9 = pd.read_csv('CarsExtractP4.csv')\n",
    "df10 = pd.read_csv('CarsExtractPFinal.csv')\n",
    "df_final = pd.concat([df, df2, df3, df4, df5, df6, df7, df8, df9, df10])\n",
    "df_final = df_final.reset_index()\n",
    "df_final.drop(['index', 'Unnamed: 0'], axis = 1, inplace = True)\n",
    "#Don't drop duplicates\n",
    "#df_final = df_final.drop_duplicates()\n",
    "df_final['Positives'] = df_final['Positives'].fillna('None mentioned')\n",
    "df_final['Negatives'] = df_final['Negatives'].fillna('None mentioned')\n",
    "df_final['Wishlist'] = df_final['Wishlist'].fillna('None mentioned')\n",
    "df_final.to_csv('CarsPass1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9373a71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfed = pd.read_csv('EdmundsPass1.csv')\n",
    "dfyt = pd.read_csv('YoutubePass1.csv')\n",
    "dfkbb = pd.read_csv('KBBPass1.csv')\n",
    "dfcars = pd.read_csv('CarsPass1.csv')\n",
    "dfPass1 = pd.concat([dfed, dfyt, dfkbb, dfcars])\n",
    "dfPass1 = dfPass1.reset_index()\n",
    "dfPass1.drop(['index', 'Unnamed: 0'], axis = 1, inplace = True)\n",
    "dfPass1 = dfPass1.dropna()\n",
    "dfPass1.to_csv('Pass1_Complete.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907850b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPass1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7693e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.read_csv('Pass1_Complete.csv')\n",
    "df_final.drop('Unnamed: 0', inplace = True, axis = 1)\n",
    "dfp = pd.DataFrame(df_final[[\"Segment\", \"Model\", \"Make\", \"Year\", \"Positives\", \"Negatives\", \"Wishlist\"]])\n",
    "dfpe = pd.DataFrame(dfp.assign(Positives = dfp['Positives'].str.split(',')).explode('Positives'))\n",
    "dfpe = pd.DataFrame(dfpe.assign(Positives = dfpe['Positives'].str.split(' and ')).explode('Positives'))\n",
    "dfpe = pd.DataFrame(dfpe.assign(Negatives = dfpe['Negatives'].str.split(',')).explode('Negatives'))\n",
    "dfpe = pd.DataFrame(dfpe.assign(Negatives = dfpe['Negatives'].str.split(' and ')).explode('Negatives'))\n",
    "dfpe = pd.DataFrame(dfpe.assign(Wishlist = dfpe['Wishlist'].str.split(',')).explode('Wishlist'))\n",
    "dfpe = pd.DataFrame(dfpe.assign(Wishlist = dfpe['Wishlist'].str.split(' and ')).explode('Wishlist'))\n",
    "dfP = dfpe[[\"Segment\", \"Make\", \"Model\", \"Year\", \"Positives\"]].reset_index()\n",
    "dfP.drop('index', axis = 1, inplace = True)\n",
    "#dfP = dfP.dropna()\n",
    "dfP = dfP.rename(columns={'Positives': 'Features'})\n",
    "dfN = dfpe[[\"Segment\", \"Make\", \"Model\", \"Year\", \"Negatives\"]].reset_index()\n",
    "dfN.drop('index', axis = 1, inplace = True)\n",
    "#dfN = dfN.dropna()\n",
    "dfN = dfN.rename(columns={'Negatives': 'Features'})\n",
    "dfW = dfpe[[\"Segment\", \"Make\",  \"Model\", \"Year\", \"Wishlist\"]].reset_index()\n",
    "dfW.drop('index', axis = 1,  inplace = True)\n",
    "#dfW = dfW.dropna()\n",
    "dfW = dfW.rename(columns={'Wishlist': 'Features'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cab7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do not drop duplicates in the Original Row Wise Exploded Positives for Frequency Analysis. \n",
    "#Note : This code is not used as processing is slightly different for Similarity/Frequency Analysis\n",
    "#dfEdmundsP.to_csv('ExplodedPositives.csv')\n",
    "#dfEdmundsN.to_csv('ExplodedNegatives.csv')\n",
    "#dfEdmundsW.to_csv('ExplodedWishlist.csv')\n",
    "\n",
    "#Remove duplicates for creating the masterlist as we don't need the duplicates for this process. \n",
    "dfP.drop_duplicates(subset = 'Features').to_csv('ExplodedPositives.csv')\n",
    "dfN.drop_duplicates(subset = 'Features').to_csv('ExplodedNegatives.csv')\n",
    "dfW.drop_duplicates(subset = 'Features').to_csv('ExplodedWishlist.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
