{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d823eccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import time \n",
    "import pdfkit\n",
    "import re\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "openai.organization = os.environ.get(\"OPENAI_ORGANIZATION\")\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "pd.set_option('display.max_rows', 500)\n",
    "openai.Model.retrieve('gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c50af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp = pd.read_csv('ExplodedPositives.csv').dropna()\n",
    "dfn = pd.read_csv('ExplodedNegatives.csv').dropna()\n",
    "dfw = pd.read_csv('ExplodedWishlist.csv').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5e0aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_condenser2(df, context):\n",
    "    master_list = []\n",
    "    index_exceptions = []\n",
    "    #Taking 500 first pass features at a time to stay with 4096 context length\n",
    "\n",
    "    features = df[\"Features\"].tolist()\n",
    "    ft = len(features)\n",
    "    #Accounting for number of features under 500, between the exact multiples of 500 etc. \n",
    "    qt = len(features) // context\n",
    "    rem = len(features) % context\n",
    "    if rem == 0:\n",
    "        iterations = qt\n",
    "    else:\n",
    "        iterations = qt + 1\n",
    "    ind = 0\n",
    "    if qt == 0:\n",
    "        ind2 = len(features)\n",
    "    else:\n",
    "        ind2 = context\n",
    "        #print('Model: ', i)\n",
    "        #print('Model Occurences: ', ft)\n",
    "        #print('Iterations', iterations)\n",
    "    for j in tqdm(range(iterations)):\n",
    "        try:\n",
    "            content = openai.ChatCompletion.create(\n",
    "            model = 'gpt-3.5-turbo',\n",
    "            messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that performs the specific task being asked.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Given a list of words in the following list which contain many words with similar semantic \"\\\n",
    "         \"meaning, condense those words into a smaller list of words which represent the same meaning. The words are related to \"\\\n",
    "         \"vehicle features. Make each word or feature into a bullet point. Focus purely on vehicle features like Fuel Efficiency, Spacious Interiors etc. \"\\\n",
    "         \"Try to make each feature as distinct as possible from the others. If similar, try to group them together into a \"\\\n",
    "         \"single feature. Avoid long bullet points. Do not number the bullets. Do not add anything else to the response like \"\\\n",
    "         \"a Note or a warning. List:  \" + (', ').join(features[ind:ind2])}\n",
    "         ],\n",
    "        temperature = 1,\n",
    "        max_tokens= 1000,\n",
    "        frequency_penalty = 0,\n",
    "        presence_penalty = 0.9,\n",
    "        #n=3\n",
    "            ).get(\"choices\")[0]['message']['content'].split('\\n')\n",
    "        \n",
    "            ret = [x.replace('- ', '') for x in content]\n",
    "        \n",
    "        #Chaining and further compression at this stage does not work well. It seems to ignore making each word/feature into a \n",
    "        #separate bullet and instead uses and to group them together which isn't what we want.\n",
    "        #content2 = openai.ChatCompletion.create(\n",
    "        #    model = 'gpt-3.5-turbo',\n",
    "        #    messages=[\n",
    "        #{\"role\": \"system\", \"content\": \"You are a helpful assistant that performs the specific task being asked.\"},\n",
    "        #{\"role\": \"user\", \"content\": \"Given a list of words in the following list which contain many words with similar semantic \"\\\n",
    "        # \"meaning, condense those words into a smaller list of words which represent the same meaning. The words are related to \"\\\n",
    "        # \"vehicle features. Do not add anything else to the response. Make each word or feature into a bullet point. \"\\\n",
    "        # \"Try to make each feature as distinct as possible from the others. If similar, try to group them together into a \"\\\n",
    "        # \"single feature. List:  \" + (', ').join(ret)}\n",
    "        # ],\n",
    "        #temperature = 1,\n",
    "        #max_tokens= 1000,\n",
    "        #frequency_penalty = 0,\n",
    "        #presence_penalty = 0.9,\n",
    "        #n=3\n",
    "        #    ).get(\"choices\")[0]['message']['content'].split('\\n')\n",
    "        \n",
    "        #ret2 = [x.replace('- ', '') for x in content2]\n",
    "                #print('Index1: ', ind)\n",
    "                #print('Index2: ', ind2)\n",
    "            ind = ind + context\n",
    "            if (ind2 + context) > ft:\n",
    "                ind2 = ft\n",
    "            else:\n",
    "                ind2 =  ind2 + context\n",
    "            master_list.append(ret)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            index_exceptions.append((ind, ind2))\n",
    "            continue\n",
    "\n",
    "        \n",
    "    #Conversion into a data format that's identical to the orginal input. i.e, a dataframe with relevant column names and no \n",
    "    #nested data structures. \n",
    "    master_list2 = []\n",
    "    for i in master_list:\n",
    "        master_list2.append(', '.join(i))\n",
    "    level1 = pd.DataFrame(master_list2, columns = [\"Features\"])\n",
    "    level1e = pd.DataFrame(level1.assign(Features = level1['Features'].str.split(',')).explode('Features'))\n",
    "    #level1e = pd.DataFrame(level1e.assign(Features = level1e['Features'].str.split(' and ')).explode('Features')) \n",
    "    level1e = pd.DataFrame(level1e.assign(Features = level1e['Features'].str.split('/')).explode('Features')) \n",
    "    level1e['Word Count'] = level1e['Features'].str.split(' ').str.len()\n",
    "    level1e = level1e.drop_duplicates()\n",
    "    return (level1e, index_exceptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aac4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "level1_masterp = semantic_condenser2(dfp, 500)\n",
    "level1_mastern = semantic_condenser2(dfn, 250)\n",
    "level1_masterw = semantic_condenser2(dfw, 250)\n",
    "level1_masterpos = level1_masterp[0]\n",
    "level1_masterneg = level1_mastern[0]\n",
    "level1_masterwish = level1_masterw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95000ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "level2_masterp = semantic_condenser2(level1_masterpos, 300)\n",
    "level2_mastern = semantic_condenser2(level1_masterneg, 300)\n",
    "level2_masterw = semantic_condenser2(level1_masterwish, 300)\n",
    "level2_masterpos = level2_masterp[0]\n",
    "level2_masterneg = level2_mastern[0]\n",
    "level2_masterwish = level2_masterw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4a1098",
   "metadata": {},
   "outputs": [],
   "source": [
    "level3_masterp = semantic_condenser2(level2_masterpos, 300)\n",
    "level3_mastern = semantic_condenser2(level2_masterneg, 300)\n",
    "level3_masterw = semantic_condenser2(level2_masterwish, 300)\n",
    "level3_masterpos = level3_masterp[0]\n",
    "level3_masterneg = level3_mastern[0]\n",
    "level3_masterwish = level3_masterw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ff7909",
   "metadata": {},
   "outputs": [],
   "source": [
    "level4_masterp = semantic_condenser2(level3_masterpos, 300)\n",
    "level4_mastern = semantic_condenser2(level3_masterneg, 300)\n",
    "level4_masterw = semantic_condenser2(level3_masterwish, 300)\n",
    "level4_masterpos = level4_masterp[0]\n",
    "level4_masterneg = level4_mastern[0]\n",
    "level4_masterwish = level4_masterw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89c5465",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only at level 5 are we able to feed the entire condensed list into a single context and completely avoid overlap\n",
    "#So level 5 is the lowest level of master list that can be considered. \n",
    "level5_masterp = semantic_condenser2(level4_masterpos, 400)\n",
    "level5_mastern = semantic_condenser2(level4_masterneg, 400)\n",
    "level5_masterw = semantic_condenser2(level4_masterwish, 400)\n",
    "level5_masterpos = level5_masterp[0]\n",
    "level5_masterneg = level5_mastern[0]\n",
    "level5_masterwish = level5_masterw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2634f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "level6_masterp = semantic_condenser2(level5_masterpos, 400)\n",
    "level6_mastern = semantic_condenser2(level5_masterneg, 400)\n",
    "level6_masterw = semantic_condenser2(level5_masterwish, 400)\n",
    "level6_masterpos = level6_masterp[0]\n",
    "level6_masterneg = level6_mastern[0]\n",
    "level6_masterwish = level6_masterw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfdf4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Very minimal change observed in level 7 hence level 6 seems to be the most optimal\n",
    "level7_masterp = semantic_condenser2(level6_masterpos, 400)\n",
    "level7_mastern = semantic_condenser2(level6_masterneg, 400)\n",
    "level7_masterw = semantic_condenser2(level6_masterwish, 400)\n",
    "level7_masterpos = level7_masterp[0]\n",
    "level7_masterneg = level7_mastern[0]\n",
    "level7_masterwish = level7_masterw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebf3f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "level5_masterpos.to_csv('CondensedPositives_L5.csv')\n",
    "level5_masterneg.to_csv('CondensedNegatives_L5.csv')\n",
    "level5_masterwish.to_csv('CondensedWishlist_L5.csv')\n",
    "level6_masterpos.to_csv('CondensedPositives_L6.csv')\n",
    "level6_masterneg.to_csv('CondensedNegatives_L6.csv')\n",
    "level6_masterwish.to_csv('CondensedWishlist_L6.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
