{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f29ab94",
   "metadata": {},
   "source": [
    "## GPT 3.5 Turbo Info Extraction Legacy Code"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2065f1ae",
   "metadata": {},
   "source": [
    "Original Copy\n",
    "\n",
    "#ChatGPT API\n",
    "def trash_to_treasure(df):\n",
    "    dflist = []\n",
    "    counter = 0\n",
    "    comments = df[\"Review\"].tolist()\n",
    "    for comment in tqdm(comments):\n",
    "        try:\n",
    "            content = openai.ChatCompletion.create(\n",
    "            model = 'gpt-3.5-turbo',\n",
    "            messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that helps answer questions and perform the specific task \"\n",
    "         + \"being asked.\"},\n",
    "        {\"role\": \"user\", \"content\":\" We are attempting to understand what vehicle features customers truly care about and what \"\\\n",
    "         + \"the pain points are based on customer comments. Note that the comments are related to \"\\\n",
    "         + \"automobile reviews hence pay more attention to certain words like Blind Spot Detection, Braking, Steering, \"\\\n",
    "         + \"Acceleration and other Car related features if they exist in the comments. Also make sure to focus only on the \"\\\n",
    "         + \"present vehicle being talked about and not on thoughts on similar vehicles owned at a prior time. With that in mind, \"\\\n",
    "         + \"what are the positives and the negative takeaways the customers had based on the following comment? Did the customer \"\\\n",
    "         + \"have any specific wishes? If so, list those out as part of a separate wishlist. Note that if the customer had a wish \"\\\n",
    "         + \"or a requested feature or doesn't list any relevant vehicle features, do not consider that as a positive or a negative. \"\\\n",
    "         + \"For each comment, formulate the answer with Positives, Negatives and Wishlist categories with short keywords per bullet. \"\\\n",
    "         + \"Do not number the bullet points. Summarize each bullet into specific keywords relating to vehicle \"\\\n",
    "         + \"features and make it a priority keep it as short as possible, around 3 - 4 words per bullet. Prefer concise words representing vehicle features over longer phrases. Do not add anything\"\\\n",
    "         + \" else to the response. Do not add an overall or summary section Wishlist. Comment : \"+comment}\n",
    "        #, {\"role\" : \"assistant\", \"content\" : \"Given the following review, here is an example of an excellent response. \n",
    "         #Review: \"+edr+\" Response: \"+ex},\n",
    "         #{\"role\" : \"user\", \"content\" : \"Based on the above example, and given requirements, what are the positives and the \n",
    "         #negative takeaways the customer had based on the following comment? Did the customer have any specific wishes? \n",
    "         #Comment: \"+ytc}\n",
    "         ],\n",
    "        temperature = 1,\n",
    "        max_tokens= 300,\n",
    "        frequency_penalty = 0,\n",
    "        presence_penalty = 0,\n",
    "        #n=3\n",
    "            ).get(\"choices\")[0]['message']['content'].split('\\n')\n",
    "    #Data Cleaning, Data Formatting into a consistent response programatically - Can only control output to an extent with \n",
    "    #prompt engineering. \n",
    "    #Blank Space Removal, Replacing Bullets with Blanks, Capturing different occurences of Categories to slice them into \n",
    "    #appropriate data formats such as list of tuples to enable creation of the final dataframe. \n",
    "            if '' in content:\n",
    "                content.remove('')\n",
    "            ret = [x.replace('- ', '') for x in content]\n",
    "            posstring = ''\n",
    "            for x in ret:\n",
    "                find = re.findall(r'^Positives:.*', x)\n",
    "                if find != []:\n",
    "                    posstring = posstring + find[0]\n",
    "                    break\n",
    "            \n",
    "            negstring = ''\n",
    "            for x in ret:\n",
    "                find = re.findall(r'^Negatives:.*', x)\n",
    "                if find != []:\n",
    "                    negstring = negstring + find[0]\n",
    "                    break\n",
    "            \n",
    "            wishstring = ''\n",
    "            for x in ret:\n",
    "                find = re.findall(r'^Wishlist:.*', x)\n",
    "                if find != []:\n",
    "                    wishstring = wishstring + find[0]\n",
    "                    break\n",
    "        \n",
    "            segment = df.iloc[counter]['Segment']\n",
    "            model = df.iloc[counter]['Model']\n",
    "            make = df.iloc[counter]['Make']\n",
    "            year = df.iloc[counter]['Year']\n",
    "            try:\n",
    "                p = ret.index(posstring)\n",
    "                n = ret.index(negstring)\n",
    "                w = ret.index(wishstring)\n",
    "                positives = ', '.join(ret[p+1:n])\n",
    "                negatives = ', '.join(ret[n+1:w])\n",
    "                wishlist = ', '.join(ret[w+1:])\n",
    "            except:\n",
    "            #This exception comes into play in case there's no relevant information in any of the categories or in context to \n",
    "            #Nissan vehicles where ChatGPT outputs a warning. This is done to maintain indexing and subsequent metadata rather\n",
    "            #than skipping the iteration entirely. \n",
    "                positives = ''\n",
    "                negatives = ''\n",
    "                wishlist = ''\n",
    "                #dflist.append((counter, segment, model, make, year, positives, negatives, wishlist))\n",
    "                #continue\n",
    "    \n",
    "            dflist.append((counter, segment, model, make, year, positives, negatives, wishlist))\n",
    "            counter = counter + 1\n",
    "            time.sleep(0.5)\n",
    "       # summarize = openai.ChatCompletion.create(\n",
    "       # model = 'gpt-3.5-turbo',\n",
    "       # messages=[\n",
    "       # {\"role\": \"system\", \"content\": \"You are a helpful assistant that detects vehicle features given a list containing descriptions of the features in question.\"},\n",
    "       # {\"role\": \"user\", \"content\":\" Parse each element in the given list and return the Nissan vehicle feature being talked about.\"\n",
    "       #  + \"Format your answer in bullet points, do not number the bullets. Maintain the Positive, Negative and Wishlist \"\n",
    "       #  + \"categories from the given list. \"\n",
    "       #  + \"For example : Infotainment technology is awesome and highly responsive. In this case, Summarize this as Infotainment\"\n",
    "       #  + \" Technology under the Positives category. For wishlists, if there is an element such as \"\n",
    "       #  + \"'more driver assist technology is beneficial', do not summarize it as driver assist technology under wishlist. \"\n",
    "       #  + \"Instead it should be More driver assist technology under the wishlist category. The degree\"\n",
    "       #  + \" of interest is important. \"\n",
    "       #  + \"List: \"+ ', '.join(ret)}\n",
    "       #  ],\n",
    "       # temperature = 1,\n",
    "       # max_tokens= 200,\n",
    "       # frequency_penalty = 0,\n",
    "       # presence_penalty = 0,\n",
    "       # #n=3\n",
    "       # ).get(\"choices\")[0]['message']['content'].split('\\n')\n",
    "    \n",
    "    #This exception is used in case of any rate errors or API connectivity issues so that continuation is possible from the \n",
    "    #remaining comments. \n",
    "        except Exception as e:\n",
    "            error = []\n",
    "            error.append(e)\n",
    "            return (dflist, e) \n",
    "    return dflist"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2370caf1",
   "metadata": {},
   "source": [
    "def semantic_condenser(df):\n",
    "    master_list = []\n",
    "    models = df['Model'].unique().tolist()\n",
    "    index_exceptions = []\n",
    "    #Taking 500 first pass features within a model at a time to stay with 4096 context length\n",
    "\n",
    "    for i in models:\n",
    "        features = df.loc[df['Model'] == i][\"Features\"].tolist()\n",
    "        ft = len(features)\n",
    "        model = i\n",
    "        make = df.loc[df['Model'] == i][\"Make\"].unique()[0]\n",
    "        segment = df.loc[df['Model'] == i][\"Segment\"].unique()[0]\n",
    "        #Accounting for number of features under 500, between the exact multiples of 500 etc. \n",
    "        qt = len(features) // 500\n",
    "        rem = len(features) % 500\n",
    "        if rem == 0:\n",
    "            iterations = qt\n",
    "        else:\n",
    "            iterations = qt + 1\n",
    "        ind = 0\n",
    "        if qt == 0:\n",
    "            ind2 = len(features)\n",
    "        else:\n",
    "            ind2 = 500\n",
    "        #print('Model: ', i)\n",
    "        #print('Model Occurences: ', ft)\n",
    "        #print('Iterations', iterations)\n",
    "        for j in range(iterations):\n",
    "            try:\n",
    "                content = openai.ChatCompletion.create(\n",
    "            model = 'gpt-3.5-turbo',\n",
    "            messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that performs the specific task being asked.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Given a list of words in the following list which contain many words with similar semantic \"\\\n",
    "         \"meaning, condense those words into a smaller list of words which represent the same meaning. The words are related to \"\\\n",
    "         \"vehicle features. Do not add anything else to the response. Make each word or feature into a bullet point. \"\\\n",
    "         \"Try to make each feature as distinct as possible from the others. If similar, try to group them together into a \"\\\n",
    "         \"single feature. List:  \" + (', ').join(features[ind:ind2])}\n",
    "         ],\n",
    "        temperature = 1,\n",
    "        max_tokens= 1000,\n",
    "        frequency_penalty = 0,\n",
    "        presence_penalty = 0.9,\n",
    "        #n=3\n",
    "            ).get(\"choices\")[0]['message']['content'].split('\\n')\n",
    "        \n",
    "                ret = [x.replace('- ', '') for x in content]\n",
    "        \n",
    "        #Chaining and further compression at this stage does not work well. It seems to ignore making each word/feature into a \n",
    "        #separate bullet and instead uses and to group them together which isn't what we want.\n",
    "        #content2 = openai.ChatCompletion.create(\n",
    "        #    model = 'gpt-3.5-turbo',\n",
    "        #    messages=[\n",
    "        #{\"role\": \"system\", \"content\": \"You are a helpful assistant that performs the specific task being asked.\"},\n",
    "        #{\"role\": \"user\", \"content\": \"Given a list of words in the following list which contain many words with similar semantic \"\\\n",
    "        # \"meaning, condense those words into a smaller list of words which represent the same meaning. The words are related to \"\\\n",
    "        # \"vehicle features. Do not add anything else to the response. Make each word or feature into a bullet point. \"\\\n",
    "        # \"Try to make each feature as distinct as possible from the others. If similar, try to group them together into a \"\\\n",
    "        # \"single feature. List:  \" + (', ').join(ret)}\n",
    "        # ],\n",
    "        #temperature = 1,\n",
    "        #max_tokens= 1000,\n",
    "        #frequency_penalty = 0,\n",
    "        #presence_penalty = 0.9,\n",
    "        #n=3\n",
    "        #    ).get(\"choices\")[0]['message']['content'].split('\\n')\n",
    "        \n",
    "        #ret2 = [x.replace('- ', '') for x in content2]\n",
    "                #print('Index1: ', ind)\n",
    "                #print('Index2: ', ind2)\n",
    "                ind = ind + 500\n",
    "            #Look into this part\n",
    "                if (ind2 + 500) > ft:\n",
    "                    ind2 = ft\n",
    "                else:\n",
    "                    ind2 =  ind2 + 500\n",
    "                master_list.append((segment, make, model, ret))\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                index_exceptions.append((model, ind, ind2))\n",
    "        \n",
    "        #print(i)\n",
    "        #print(len(ret))\n",
    "        #print(ind2)\n",
    "        \n",
    "    #Conversion into a data format that's identical to the orginal input. i.e, a dataframe with relevant column names and no \n",
    "    #nested data structures. \n",
    "    level1 = pd.DataFrame(master_list, columns = [\"Segment\", \"Make\", \"Model\", \"Features\"])\n",
    "    for i in range(len(level1)):\n",
    "        level1[\"Features\"][i] = ', '.join(level1[\"Features\"][i])\n",
    "    level1e = pd.DataFrame(level1.assign(Features = level1['Features'].str.split(',')).explode('Features'))\n",
    "    level1e = pd.DataFrame(level1e.assign(Features = level1e['Features'].str.split(' and ')).explode('Features'))\n",
    "    level1e = level1e.drop_duplicates().dropna().reset_index()\n",
    "    level1e.drop('index', axis = 1, inplace = True)\n",
    "    return level1e"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a00bdc78",
   "metadata": {},
   "source": [
    "#Issue of mashing comments can be seen here. It gives an aggregate and we won't be able to tell if \n",
    "#for eg. many people hate transmissions or is it just one person's opinion? etc. \n",
    "a = trash_to_treasure([commentmash])\n",
    "a"
   ]
  },
  {
   "cell_type": "raw",
   "id": "65efdcc6",
   "metadata": {},
   "source": [
    "#regex exception capture\n",
    "strings = ['-hello-', '- world', '- none', '-none-', '-none -', 'hello']\n",
    "\n",
    "# search for strings starting and ending with '-', starting with '- ', or containing '-none-'\n",
    "pattern = re.compile(r'^-.*-$|^-\\s.*|^-\\bnone\\b- |^-none- $')\n",
    "matches = [string for string in strings if pattern.search(string)]\n",
    "\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f0796b3e",
   "metadata": {},
   "source": [
    "ytreviews = pd.read_csv('YoutubeComments.csv')\n",
    "#Wont be using mashed up comments as it wouldn be hard to distinguish relative feature demand/pain points\n",
    "#The string encoding issues don't need to be cleaned. GPT3.5Turbo can handle those just fine.\n",
    "commentmash = ''\n",
    "for i in range(70, 80):\n",
    "    c = ytreviews[\"Comments\"][i]\n",
    "    commentmash = commentmash+c+' '\n",
    "    \n",
    "commentmash"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ff2b428b",
   "metadata": {},
   "source": [
    "Batching comments does not work in a consistent way (to improve speed). It's an extra layer of complexity of top of the already stochastic process. Hence to improve computation time, Comment dataframes will be separated and run in parallel on different runtimes instead as the computation in regards to inference is on the OpenAI end. \n",
    "\n",
    "comment_batch = []\n",
    "l1 = edmundsreviews[\"Review\"][0:2].tolist()\n",
    "for i in range(len(l1)):\n",
    "    comment_batch.append((\"Comment \"+str(i+1)+': '+l1[i]))\n",
    "batched = [', '.join(comment_batch)]\n",
    "batched"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1ab4f7cf",
   "metadata": {},
   "source": [
    "#Batching attempt - Information Rich Youtube Comments were used instead. (Over 40 words) \n",
    "\n",
    "gcount = df['Model'].value_counts().tolist()\n",
    "print(gcount)\n",
    "ind = 0\n",
    "for i in range(0, len(gcount)):\n",
    "    rem = gcount[i]%5\n",
    "    if rem != 0:\n",
    "        df.drop(df.index[gcount[i]-rem : gcount[i]], inplace = True)\n",
    "        print(gcount[i]-rem, gcount[i])\n",
    "    ind = gcount[i] + ind\n",
    "    print(ind)\n",
    "\n",
    "#sampled_indexes = df.groupby('Model').apply(lambda x: x.sample(n=2)).index.get_level_values(1)\n",
    "sampled_indexes = df.groupby('Model').get_group('Honda Civic 2022').apply(lambda x: x.sample(n=3)).index.tolist()\n",
    "sampled_indexes\n",
    "#df.groupby('Model').get_group()\n",
    "def sample_index_group:\n",
    "    sampled_indexes = []\n",
    "    remi = []\n",
    "    for i in df['Model'].unique().tolist():\n",
    "        rem = df.groupby('Model').get_group(i)['Model'].value_counts().tolist()[0]%5\n",
    "        #Sampling is erratic, while it's supposed to sample in this case 2 per group, it doesn't quite work that way so \n",
    "        #a workaround is used. \n",
    "        sampled_indexes.append(df.groupby('Model').get_group(i).apply(lambda x: x.sample(n=2)).index.tolist())\n",
    "        remi.append(rem)\n",
    "        l = []\n",
    "        for i in sampled_indexes:\n",
    "            l.append(len(i))\n",
    "        if min(l) >= max(remi):\n",
    "            \n",
    "    \n",
    "#def sample_rows(group):\n",
    "#    num_samples = np.random.randint(1, len(group) + 1)\n",
    "#    return group.sample(n=num_samples).index\n",
    "\n",
    "# group the DataFrame by 'group', and select a random number of rows from each group\n",
    "#sampled_indexes = df.groupby('group').apply(sample_rows).explode().values\n",
    "\n",
    "# print the sampled indexes\n",
    "#print(sampled_indexes)\n",
    "df.drop(df.index[list(sampled_indexes)])['Model'].value_counts()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64539d95",
   "metadata": {},
   "source": [
    "## Semantic Condenser"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b2ccf71f",
   "metadata": {},
   "source": [
    "def semantic_condenser(df):\n",
    "    master_list = []\n",
    "    index_exceptions = []\n",
    "    #Taking 500 first pass features at a time to stay with 4096 context length\n",
    "\n",
    "    features = df[\"Features\"].tolist()\n",
    "    ft = len(features)\n",
    "    #Accounting for number of features under 500, between the exact multiples of 500 etc. \n",
    "    qt = len(features) // 500\n",
    "    rem = len(features) % 500\n",
    "    if rem == 0:\n",
    "        iterations = qt\n",
    "    else:\n",
    "        iterations = qt + 1\n",
    "    ind = 0\n",
    "    if qt == 0:\n",
    "        ind2 = len(features)\n",
    "    else:\n",
    "        ind2 = 500\n",
    "        #print('Model: ', i)\n",
    "        #print('Model Occurences: ', ft)\n",
    "        #print('Iterations', iterations)\n",
    "    for j in range(iterations):\n",
    "        try:\n",
    "            content = openai.ChatCompletion.create(\n",
    "            model = 'gpt-3.5-turbo',\n",
    "            messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that performs the specific task being asked.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Given a list of words in the following list which contain many words with similar semantic \"\\\n",
    "         \"meaning, condense those words into a smaller list of words which represent the same meaning. The words are related to \"\\\n",
    "         \"vehicle features. Make each word or feature into a bullet point. Focus purely on vehicle features like Fuel Efficiency, Spacious Interiors etc. \"\\\n",
    "         \"Try to make each feature as distinct as possible from the others. If similar, try to group them together into a \"\\\n",
    "         \"single feature. Avoid long bullet points. Do not number the bullets. Do not add anything else to the response like \"\\\n",
    "         \"a Note or a warning. List:  \" + (', ').join(features[ind:ind2])}\n",
    "         ],\n",
    "        temperature = 1,\n",
    "        max_tokens= 1000,\n",
    "        frequency_penalty = 0,\n",
    "        presence_penalty = 0.9,\n",
    "        #n=3\n",
    "            ).get(\"choices\")[0]['message']['content'].split('\\n')\n",
    "        \n",
    "            ret = [x.replace('- ', '') for x in content]\n",
    "        \n",
    "        #Chaining and further compression at this stage does not work well. It seems to ignore making each word/feature into a \n",
    "        #separate bullet and instead uses and to group them together which isn't what we want.\n",
    "        #content2 = openai.ChatCompletion.create(\n",
    "        #    model = 'gpt-3.5-turbo',\n",
    "        #    messages=[\n",
    "        #{\"role\": \"system\", \"content\": \"You are a helpful assistant that performs the specific task being asked.\"},\n",
    "        #{\"role\": \"user\", \"content\": \"Given a list of words in the following list which contain many words with similar semantic \"\\\n",
    "        # \"meaning, condense those words into a smaller list of words which represent the same meaning. The words are related to \"\\\n",
    "        # \"vehicle features. Do not add anything else to the response. Make each word or feature into a bullet point. \"\\\n",
    "        # \"Try to make each feature as distinct as possible from the others. If similar, try to group them together into a \"\\\n",
    "        # \"single feature. List:  \" + (', ').join(ret)}\n",
    "        # ],\n",
    "        #temperature = 1,\n",
    "        #max_tokens= 1000,\n",
    "        #frequency_penalty = 0,\n",
    "        #presence_penalty = 0.9,\n",
    "        #n=3\n",
    "        #    ).get(\"choices\")[0]['message']['content'].split('\\n')\n",
    "        \n",
    "        #ret2 = [x.replace('- ', '') for x in content2]\n",
    "                #print('Index1: ', ind)\n",
    "                #print('Index2: ', ind2)\n",
    "            ind = ind + 500\n",
    "            if (ind2 + 500) > ft:\n",
    "                ind2 = ft\n",
    "            else:\n",
    "                ind2 =  ind2 + 500\n",
    "            master_list.append(ret)\n",
    "            \n",
    "        except Exception as e:\n",
    "            #print(e)\n",
    "            try: \n",
    "                qt = len(features) // 250\n",
    "                rem = len(features) % 250\n",
    "                if rem == 0:\n",
    "                    iterations = qt\n",
    "                else:\n",
    "                    iterations = qt + 1\n",
    "                ind = 0\n",
    "                if qt == 0:\n",
    "                    ind2 = len(features)\n",
    "                else:\n",
    "                    ind2 = 250\n",
    "                for j in range(iterations):\n",
    "                    content = openai.ChatCompletion.create(\n",
    "                    model = 'gpt-3.5-turbo',\n",
    "                    messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant that performs the specific task being asked.\"},\n",
    "                    {\"role\": \"user\", \"content\": \"Given a list of words in the following list which contain many words with similar semantic \"\\\n",
    "                     \"meaning, condense those words into a smaller list of words which represent the same meaning. The words are related to \"\\\n",
    "                     \"vehicle features. Make each word or feature into a bullet point. Focus purely on vehicle features like Fuel Efficiency, Spacious Interiors etc. \"\\\n",
    "                     \"Try to make each feature as distinct as possible from the others. If similar, try to group them together into a \"\\\n",
    "                     \"single feature. Avoid long bullet points. Do not number the bullets. Do not add anything else to the response like a Note or a warning. \"\\\n",
    "                     \"List:  \" + (', ').join(features[ind:ind2])}\n",
    "                    ],\n",
    "                    temperature = 1,\n",
    "                    max_tokens= 1000,\n",
    "                    frequency_penalty = 0,\n",
    "                    presence_penalty = 0.9,\n",
    "                    #n=3\n",
    "                    ).get(\"choices\")[0]['message']['content'].split('\\n')\n",
    "                    ret = [x.replace('- ', '') for x in content]\n",
    "                    ind = ind + 250\n",
    "                    if (ind2 + 250) > ft:\n",
    "                        ind2 = ft\n",
    "                    else:\n",
    "                        ind2 =  ind2 + 250\n",
    "                    master_list.append(ret)\n",
    "            except:\n",
    "                print(e)\n",
    "                index_exceptions.append((ind, ind2))\n",
    "\n",
    "        \n",
    "    #Conversion into a data format that's identical to the orginal input. i.e, a dataframe with relevant column names and no \n",
    "    #nested data structures. \n",
    "    master_list2 = []\n",
    "    for i in master_list:\n",
    "        master_list2.append(', '.join(i))\n",
    "    level1 = pd.DataFrame(master_list2, columns = [\"Features\"])\n",
    "    level1e = pd.DataFrame(level1.assign(Features = level1['Features'].str.split(',')).explode('Features'))\n",
    "    #level1e = pd.DataFrame(level1e.assign(Features = level1e['Features'].str.split(' and ')).explode('Features')) \n",
    "    level1e = pd.DataFrame(level1e.assign(Features = level1e['Features'].str.split('/')).explode('Features')) \n",
    "    level1e['Word Count'] = level1e['Features'].str.split(' ').str.len()\n",
    "    level1e = level1e.drop_duplicates()\n",
    "    return (level1e, index_exceptions)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a40f0966",
   "metadata": {},
   "source": [
    "#The semantic condenser is a multi-step process with the final condensed word list formed after 3 levels of semantic grouping\n",
    "#where the output of level 1 is given as input to level 2 and so on. Levels 4 and 5 are optional. \n",
    "level1_master = semantic_condenser(dfp)[0]\n",
    "level2_master = semantic_condenser(level1_master)[0]\n",
    "level3_master = semantic_condenser(level2_master)[0]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d7ad9abb",
   "metadata": {},
   "source": [
    "#Below processing wasn't required\n",
    "def strip_string(s):\n",
    "    return s.strip()\n",
    "def strip_hyphen(s):\n",
    "    return s.strip('-')\n",
    "level4_master['Features'] = level4_master['Features'].apply(strip_string)\n",
    "level4_master['Features'] = level4_master['Features'].apply(strip_hyphen)\n",
    "level4_master['Features'] = level4_master['Features'].str.replace(r'^\\d+\\.\\s*', '')\n",
    "level4_master = level4_master[~level4_master['Features'].str.contains('Note: ')]\n",
    "#level4_master.to_csv('EdmundsPositivesMasterList_L4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d06b9eb",
   "metadata": {},
   "source": [
    "## Edmunds Scraping"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f6942305",
   "metadata": {},
   "source": [
    "Legacy Code\n",
    "\n",
    "year_list = ['2020', '2021']\n",
    "car_list = ['Altima', 'Sentra', 'Maxima','Versa']\n",
    "sportscar_list = ['Z', 'GT-R']\n",
    "suv_list = ['Murano', 'Rogue', 'Armada', 'Pathfinder', 'Kicks', 'Rogue-Sport']\n",
    "trucks_list = ['Frontier', 'Frontier-Crew-Cab', 'Frontier-King-Cab' 'Titan', 'Titan-Crew-Cab', 'Titan-King-Cab', 'Titan-XD']\n",
    "dic2 = {'Car' : [i for i in car_list], 'SUV' : [i for i in suv_list], 'Truck' : [i for i in trucks_list], 'Sports Car' : [i for i in sportscar_list]}\n",
    "models = car_list + suv_list + trucks_list\n",
    "\n",
    "\n",
    "t = time.time()\n",
    "data = []\n",
    "reviews_df = pd.DataFrame(columns = [\"Model\", \"Year\", \"Type\", \"Reviews\"])\n",
    "#iterate through years\n",
    "comp = 1\n",
    "for j in year_list:\n",
    "    #iterate through models\n",
    "    for k in models:\n",
    "        print(comp)\n",
    "        print(time.time() -t)\n",
    "        print(j,k)\n",
    "        comp = comp + 1\n",
    "        try:\n",
    "            counter = 0\n",
    "            #page navigation for single model \n",
    "            for i in itertools.count():\n",
    "                counter = counter+1\n",
    "                url = 'https://www.edmunds.com/nissan/'+k+'/'+str(j)+'/consumer-reviews/?pagenum='+str(counter)+''\n",
    "                headers = {\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"}\n",
    "                page = requests.get(url, timeout = 5, headers = headers)\n",
    "                #print('Yes')\n",
    "                if page.status_code == 200:\n",
    "                    for q, w in dic2.items():\n",
    "                        if str.lower(k) in [str.lower(p) for p in w]:\n",
    "                            Type = q\n",
    "                    #print('Dict')\n",
    "                    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "                    reviews=soup.find_all('p')\n",
    "                    #print('Parsed for one Page')\n",
    "                    #For 2022 onwards use below\n",
    "                    #if \"Receive pricing updates, shopping tips &amp; more!\" in str(reviews[0]):\n",
    "                    #Before 2022 use below\n",
    "                    if \"Download the Edmunds app\" in str(reviews[4]):\n",
    "                        break\n",
    "                    for i in range(len(reviews)):\n",
    "                        data.append(('Nissan '+k, j, Type, reviews[i].get_text()))\n",
    "                    #print('Appended for 1 page')\n",
    "                    #print(counter)\n",
    "                    #print(data[-3:])\n",
    "                    time.sleep(2)\n",
    "                else:\n",
    "                    break\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "df[\"Type\"].mask(df[\"Model\"].isin(['Nissan '+i for i in car_list]), 'Car', inplace = True)\n",
    "df[\"Type\"].mask(df[\"Model\"].isin(['Nissan '+i for i in suv_list]), 'SUV', inplace = True)\n",
    "df[\"Type\"].mask(df[\"Model\"].isin(['Nissan '+i for i in trucks_list]), 'Truck', inplace = True)\n",
    "df[\"Type\"].mask(df[\"Model\"].isin(['Nissan '+i for i in sportscar_list]), 'Sports Car', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b3e7f7",
   "metadata": {},
   "source": [
    "## Youtube Scraping "
   ]
  },
  {
   "cell_type": "raw",
   "id": "18ca729d",
   "metadata": {},
   "source": [
    "Legacy Code\n",
    "\n",
    "video_ids_dict = {'Nissan Altima 2023' : ['gmoCi5mfhJg', '_l9VGoAFEys', '4kiT84t-aME', '5COGmt48ujE', 'OhlhXibDYDQ'], \n",
    "             'Nissan Altima 2022' : ['fHwyLMQe3TA', 'YjexTNW7f7Q'],\n",
    "            'Nissan Sentra 2022' : ['PP4_LVtgRJk', 'Gpic4TKRemA'],\n",
    "            'Nissan Versa 2022' : ['jwMAgmSBrh8', 'B0w8e7r7VDM'],\n",
    "            'Nissan Maxima 2023' : ['lbmBH8nGUNA', 'RzV5gyQfqcA'],\n",
    "            'Nissan Maxima 2022' : ['EoSqbXMzcsY', '_7vmQ97H_kc'],\n",
    "            'Nissan Rogue 2023' : ['7kaMFc83jKE', 'laCQ4hyfIfc'],\n",
    "            'Nissan Rogue 2022' : ['yUb_JUdKxLY', 'xxCM_QQYsac'], \n",
    "            'Nissan Murano 2023' : ['huJvlzOJYnc', 'Zo6gdOlOYAw'],\n",
    "            'Nissan Murano 2022' : ['Occ4jrNbrZA', 'J615oF0NkWg'],\n",
    "             'Nissan Armada 2023' : ['I2alYZocsVE', 'QECtK57GArY', 'ORkMTKz6rQc'],\n",
    "            'Nissan Armada 2022' : ['4VwdXRDB7kA', 'GDOWorHWfZU'],\n",
    "             'Nissan Kicks 2023' : ['VqqReOXX88k', 'nwimw4lIc6k', 'hQwlnLePOT8'],\n",
    "             'Nissan Titan 2023' : ['cg4DUro9pws', 'ybofvWeei9o', 'wmgjjm8oEb0'],\n",
    "             'Nissan Frontier 2023' : ['dCJ6aBvpW9o'],\n",
    "            'Nissan Frontier 2022' : ['h8PxavrpaWc', 'Rfaqj5dMuU4', 'oSFhdNi-qGc', 'JSWPEtlrVPA', '9hl8n_Bj9eg'],\n",
    "             'Nissan Frontier Crew Cab 2023' : ['nBwJnycmcD8'], \n",
    "             'Nissan Pathfinder 2023' : ['CTrq4VEr-RQ', 'GYrsgAIY35U', 'As5zh538vvg'],\n",
    "            'Nissan Pathfinder 2022' : ['ZCgwptGHJXA', '26_Btfr-RgA', 'B-SlReR_fqo', 'AArAjAt8gnI'],\n",
    "            'Nissan Z 2023' : ['1ciacF7ZshI', 'Y5D-4JyG2dA'], \n",
    "            'Nissan GT-R 2023' : ['yPprJs6el9I'],\n",
    "            'Nissan Car Comparison' : ['YjexTNW7f7Q']}\n",
    "\n",
    "video_ids2 = []\n",
    "for i, j in video_ids_dict.items():\n",
    "    video_ids2.append(j)\n",
    "\n",
    "car_list = ['Altima', 'Sentra', 'Maxima','Versa']\n",
    "sportscar_list = ['Z', 'GT-R']\n",
    "suv_list = ['Murano', 'Rogue', 'Armada', 'Pathfinder', 'Kicks', 'Rogue-Sport']\n",
    "trucks_list = ['Frontier', 'Frontier-Crew-Cab', 'Frontier-King-Cab' 'Titan', 'Titan-Crew-Cab', 'Titan-King-Cab', 'Titan-XD', 'Titan_XD']    \n",
    "dic2 = {'Car' : [i for i in car_list], 'SUV' : [i for i in suv_list], 'Truck' : [i for i in trucks_list], 'Sports Car' : [i for i in sportscar_list]}\n",
    "video_ids2 = [video_id for sublist in video_ids2 for video_id in sublist]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
